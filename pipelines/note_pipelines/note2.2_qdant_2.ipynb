{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Qdrant search...\n",
      "Response Status: 200\n",
      "Response Data: {'result': [{'id': '7c9b8dbf-e631-429d-985d-b443b04a9dad', 'version': 0, 'score': 0.056992024}, {'id': 'a3fc7706-2812-4495-838e-18cbcd5bba89', 'version': 0, 'score': 0.04741749}, {'id': '6a20096d-8342-4b6f-8860-6ac2bc0ad306', 'version': 1, 'score': 0.04589819}, {'id': 'e28045df-f779-462f-a4e7-2d5a7ca66166', 'version': 1, 'score': 0.045791358}, {'id': 'a5693358-ca3e-4d1d-9d4c-b383348a989d', 'version': 2, 'score': 0.04545533}], 'status': 'ok', 'time': 0.001576648}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "# Cấu hình Qdrant\n",
    "QDRANT_API_URL = os.getenv(\n",
    "    \"QDRANT_API_URL\",\n",
    "    \"https://fcbf96b5-0f95-47b1-b088-dd1eba2a2758.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
    ")\n",
    "QDRANT_API_KEY = os.getenv(\n",
    "    \"QDRANT_API_KEY\", \"WbQ_8KeZKchBfQ-atnt5zfbkIShw6slMNvF0PK8qIOEIgaYqTyZLmw\"\n",
    ")\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION\", \"cmc_final_db\")\n",
    "\n",
    "\n",
    "def test_qdrant_search():\n",
    "    \"\"\"\n",
    "    Gửi một truy vấn đơn giản tới Qdrant để kiểm tra kết nối và response.\n",
    "    \"\"\"\n",
    "    url = f\"{QDRANT_API_URL}/collections/{QDRANT_COLLECTION}/points/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {QDRANT_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # Tạo một vector ngẫu nhiên kích thước 768\n",
    "    query_vector = list(np.random.rand(768))  # Hoặc vector thực tế từ pipeline của bạn\n",
    "    payload = {\n",
    "        \"vector\": query_vector,\n",
    "        \"limit\": 5,  # Số lượng kết quả muốn lấy\n",
    "    }\n",
    "\n",
    "    print(\"Testing Qdrant search...\")\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Response Status: {response.status_code}\")\n",
    "        print(\"Response Data:\", response.json())\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(\"HTTP Error:\", e.response.status_code, e.response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred:\", str(e))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_qdrant_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add thêm Embedding Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query...\n",
      "Generated embedding: [-0.0054861484, -0.006567113, 0.010118369, -0.023354253, -0.010823199]...\n",
      "Testing Qdrant search...\n",
      "HTTP Error: 400 {\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 768, got 1536\"},\"time\":0.000635589}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Qdrant configuration\n",
    "QDRANT_API_URL = os.getenv(\"QDRANT_API_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Thay bằng API key của bạn\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION\")\n",
    "\n",
    "# OpenAI embedding configuration (nếu dùng OpenAI)\n",
    "OPENAI_API_URL = os.getenv(\"OPENAI_API_URL\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # Thay bằng API key OpenAI của bạn\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")  # Model embedding từ OpenAI\n",
    "\n",
    "\n",
    "# Function to generate embedding using OpenAI\n",
    "def generate_embedding(query_text):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"input\": query_text,\n",
    "        \"model\": \"text-embedding-ada-002\",\n",
    "    }\n",
    "\n",
    "    print(\"Generating embedding for query...\")\n",
    "    response = requests.post(OPENAI_API_URL, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# Function to search in Qdrant\n",
    "def search_qdrant(embedding_vector):\n",
    "    url = f\"{QDRANT_API_URL}/collections/{QDRANT_COLLECTION}/points/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {QDRANT_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"vector\": embedding_vector,\n",
    "        \"limit\": 5,  # Số lượng kết quả muốn lấy\n",
    "    }\n",
    "\n",
    "    print(\"Testing Qdrant search...\")\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Main flow\n",
    "def main():\n",
    "    # Query text\n",
    "    query_text = \"Find information about deep learning and AI models.\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Generate embedding\n",
    "        embedding_vector = generate_embedding(query_text)\n",
    "        print(f\"Generated embedding: {embedding_vector[:5]}...\")  # In 5 giá trị đầu\n",
    "\n",
    "        # Step 2: Search in Qdrant\n",
    "        search_results = search_qdrant(embedding_vector)\n",
    "        print(\"Search Results:\", search_results)\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(\"HTTP Error:\", e.response.status_code, e.response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred:\", str(e))\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Generating embedding for query...\n",
    "Generated embedding: [-0.0054861484, -0.006567113, 0.010118369, -0.023354253, -0.010823199]...\n",
    "Testing Qdrant search...\n",
    "HTTP Error: 400 {\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 768, got 1536\"},\"time\":0.000876167}\n",
    "```\n",
    "\n",
    "Đảm bảo rằng MODEL_NAME trong .env là mô hình phù hợp. Ví dụ:\n",
    "text-embedding-ada-002 (kích thước 1536).\n",
    "text-similarity-babbage-001 (kích thước 768).\n",
    "\n",
    "- Nếu muốn dùng OpenAI text-embedding-ada-002, thì chỉnh Qdrant vector size thành 1536.\n",
    "- Nếu muốn giữ Qdrant vector size là 768, thì đổi sang model OpenAI tương ứng (như text-similarity-babbage-001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Khi nào chọn 768 hoặc 1536?**\n",
    "| **Tiêu chí**          | **Vector 768**                                    | **Vector 1536**                                   |\n",
    "|-----------------------|--------------------------------------------------|--------------------------------------------------|\n",
    "| **Ứng dụng**          | Tìm kiếm cơ bản, dữ liệu nhỏ, bài toán đơn giản   | Tìm kiếm phức tạp, dữ liệu lớn, bài toán yêu cầu cao |\n",
    "| **Tài nguyên hệ thống** | Hạn chế về bộ nhớ hoặc tài nguyên tính toán       | Tài nguyên đủ mạnh để xử lý kích thước lớn       |\n",
    "| **Độ chính xác**       | Không yêu cầu cực cao, chấp nhận sai số nhỏ       | Yêu cầu chính xác cao và biểu diễn ngữ nghĩa sâu |\n",
    "| **Chi phí**           | Tiết kiệm chi phí lưu trữ và xử lý               | Sẵn sàng chi phí cao hơn để đạt hiệu quả tốt hơn |\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm lại**:\n",
    "- **768**: Lựa chọn tốt nếu bạn ưu tiên tốc độ, tài nguyên hạn chế, và bài toán không quá phức tạp.\n",
    "- **1536**: Phù hợp khi bạn muốn tối ưu độ chính xác và có đủ tài nguyên để xử lý bài toán phức tạp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng model BERT 768 (thay vì openAI 1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for query...\n",
      "Generated embedding: [-0.14006862044334412, 0.12466755509376526, 0.002916180994361639, 0.06585323065519333, 0.056434180587530136]...\n",
      "Testing Qdrant search...\n",
      "Search Results: {'result': [{'id': '6e28e1cf-83d2-4c59-9824-d3d21a83bfeb', 'version': 0, 'score': 0.15543127}, {'id': 'b01a0f6c-4f08-495f-9986-00d89df298cb', 'version': 1, 'score': 0.14606589}, {'id': 'ada60796-74d4-4e16-b081-5cf8ba70a787', 'version': 1, 'score': 0.13553998}, {'id': 'c28a734b-1530-47a5-81a3-2c969404106d', 'version': 0, 'score': 0.13342223}, {'id': 'bef4d151-b14e-43ae-826a-80aefc9c412f', 'version': 2, 'score': 0.13268083}], 'status': 'ok', 'time': 0.000981311}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Hugging Face configuration\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "EMBEDDINGS_MODEL_NAME = os.getenv(\"EMBEDDINGS_MODEL_NAME\")\n",
    "\n",
    "# Qdrant configuration\n",
    "QDRANT_API_URL = os.getenv(\"QDRANT_API_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION\")\n",
    "\n",
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    model_name=EMBEDDINGS_MODEL_NAME,\n",
    "    api_key=HUGGINGFACE_API_KEY,\n",
    "    model_kwargs={'device': 'auto'}\n",
    ")\n",
    "\n",
    "# Function to generate embedding using Hugging Face\n",
    "def generate_embedding(query_text):\n",
    "    print(\"Generating embedding for query...\")\n",
    "    embedding_vector = embeddings.embed_query(query_text)\n",
    "    return embedding_vector\n",
    "\n",
    "# Function to search in Qdrant\n",
    "def search_qdrant(embedding_vector):\n",
    "    url = f\"{QDRANT_API_URL}/collections/{QDRANT_COLLECTION}/points/search\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {QDRANT_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"vector\": embedding_vector,\n",
    "        \"limit\": 5,  # Number of results to retrieve\n",
    "    }\n",
    "\n",
    "    print(\"Testing Qdrant search...\")\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Main flow\n",
    "def main():\n",
    "    # Query text\n",
    "    query_text = \"Find information about deep learning and AI models.\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Generate embedding\n",
    "        embedding_vector = generate_embedding(query_text)\n",
    "        print(f\"Generated embedding: {embedding_vector[:5]}...\")  # Print first 5 values\n",
    "\n",
    "        # Step 2: Search in Qdrant\n",
    "        search_results = search_qdrant(embedding_vector)\n",
    "        print(\"Search Results:\", search_results)\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(\"HTTP Error:\", e.response.status_code, e.response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred:\", str(e))\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ver 1 pipeline: \n",
    "\n",
    "```python \n",
    "\n",
    "\"\"\"\n",
    "title: Qdrant Cloud Search Pipeline\n",
    "author: YourName\n",
    "date: 2025-01-10\n",
    "version: 1.0\n",
    "license: MIT\n",
    "description: A pipeline to interact with Qdrant Cloud for vector search.\n",
    "requirements: requests\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from typing import List, Union, Generator, Iterator\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    class Valves(BaseModel):\n",
    "        QDRANT_API_URL: str\n",
    "        QDRANT_API_KEY: str\n",
    "        QDRANT_COLLECTION: str\n",
    "\n",
    "    def __init__(self):\n",
    "        self.valves = self.Valves(\n",
    "            **{\n",
    "                \"QDRANT_API_URL\": os.getenv(\"QDRANT_API_URL\", \"https://fcbf96b5-0f95-47b1-b088-dd1eba2a2758.us-east4-0.gcp.cloud.qdrant.io:6333\"),\n",
    "                \"QDRANT_API_KEY\": os.getenv(\"QDRANT_API_KEY\", \"WbQ_8KeZKchBfQ-atnt5zfbkIShw6slMNvF0PK8qIOEIgaYqTyZLmw\"),\n",
    "                \"QDRANT_COLLECTION\": os.getenv(\"QDRANT_COLLECTION\", \"cmc_final_db\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    async def on_startup(self):\n",
    "        print(\"Qdrant Cloud Pipeline started.\")\n",
    "\n",
    "    async def on_shutdown(self):\n",
    "        print(\"Qdrant Cloud Pipeline stopped.\")\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_k: int = 5) -> dict:\n",
    "        \"\"\"\n",
    "        Search Qdrant collection for nearest neighbors to the query vector.\n",
    "        \"\"\"\n",
    "        url = f\"{self.valves.QDRANT_API_URL}/collections/{self.valves.QDRANT_COLLECTION}/points/search\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.valves.QDRANT_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"vector\": query_vector,\n",
    "            \"limit\": top_k,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()  # Assuming API returns JSON\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error querying Qdrant: {e}\")\n",
    "            return {\"error\": \"Unable to query Qdrant Cloud\"}\n",
    "\n",
    "    def pipe(\n",
    "        self, user_message: str, model_id: str, messages: List[dict], body: dict\n",
    "    ) -> Union[str, Generator, Iterator]:\n",
    "        \"\"\"\n",
    "        Process user message and query Qdrant for vector search.\n",
    "        \"\"\"\n",
    "        print(f\"User message: {user_message}\")\n",
    "\n",
    "        # Convert the user message to a query vector (dummy example, replace with real embedding logic)\n",
    "        query_vector = [0.1, 0.2, 0.3, 0.4, 0.5]  # Replace with embedding generation logic\n",
    "\n",
    "        # Search in Qdrant\n",
    "        qdrant_response = self.search_vectors(query_vector)\n",
    "\n",
    "        # Process response\n",
    "        if \"error\" in qdrant_response:\n",
    "            return qdrant_response[\"error\"]\n",
    "\n",
    "        # Format the results\n",
    "        results = qdrant_response.get(\"result\", [])\n",
    "        if not results:\n",
    "            return \"No relevant data found in Qdrant Cloud.\"\n",
    "\n",
    "        formatted_results = \"\\n\".join([f\"- ID: {item['id']}, Score: {item['score']}\" for item in results])\n",
    "        return f\"Here are the top results from Qdrant Cloud:\\n\\n{formatted_results}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
