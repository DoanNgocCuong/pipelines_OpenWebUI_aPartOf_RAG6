{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example of pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llamaindex_ollama_github_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là một pipeline (đường ống xử lý) sử dụng Llama Index và Ollama để tạo một hệ thống RAG (Retrieval Augmented Generation) làm việc với dữ liệu từ kho GitHub.\n",
    "\n",
    "Các thành phần chính:\n",
    "\n",
    "1. **Khởi tạo (on_startup)**:\n",
    "- Sử dụng Ollama để tạo embeddings và LLM\n",
    "- Kết nối với GitHub repository để lấy dữ liệu\n",
    "- Tạo index vector từ các tài liệu\n",
    "\n",
    "2. **Xử lý (pipe)**:\n",
    "- Nhận tin nhắn từ người dùng\n",
    "- Sử dụng query engine để tìm thông tin liên quan\n",
    "- Trả về câu trả lời được tạo ra\n",
    "\n",
    "3. **Cấu hình**:\n",
    "- Sử dụng model \"nomic-embed-text\" cho embeddings\n",
    "- Sử dụng model \"llama3\" cho LLM\n",
    "- Lọc các file không cần thiết (.png, .jpg, v.v.)\n",
    "\n",
    "Pipeline này cho phép chatbot trả lời câu hỏi dựa trên thông tin từ repository GitHub được chỉ định, kết hợp giữa việc tìm kiếm thông tin (retrieval) và tạo câu trả lời (generation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llamaindex_ollama_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "File đầu tiên là một pipeline tương tự nhưng làm việc với dữ liệu từ thư mục local thay vì GitHub.\n",
    "\n",
    "Các điểm chính:\n",
    "\n",
    "1. **Cấu trúc**:\n",
    "- Sử dụng `SimpleDirectoryReader` để đọc dữ liệu từ thư mục `/app/backend/data`\n",
    "- Cũng sử dụng Ollama cho embeddings và LLM\n",
    "\n",
    "2. **Cấu hình linh hoạt hơn**:\n",
    "- Có class `Valves` để quản lý các biến cấu hình\n",
    "- Các thông số có thể được điều chỉnh qua biến môi trường:\n",
    "  ```python\n",
    "  LLAMAINDEX_OLLAMA_BASE_URL\n",
    "  LLAMAINDEX_MODEL_NAME\n",
    "  LLAMAINDEX_EMBEDDING_MODEL_NAME\n",
    "  ```\n",
    "\n",
    "3. **Chức năng tương tự**:\n",
    "- Tạo vector index từ tài liệu\n",
    "- Xử lý câu hỏi người dùng\n",
    "- Trả về câu trả lời dạng stream\n",
    "\n",
    "So với pipeline GitHub:\n",
    "- Đơn giản hơn vì không cần xử lý kết nối GitHub\n",
    "- Linh hoạt hơn trong việc cấu hình\n",
    "- Phù hợp khi dữ liệu được lưu trữ locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llamaindex_pipeline.py - Pipeline cơ bản sử dụng OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_to_sql_pipeline.py - Pipeline sử dụng database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "À, bây giờ tôi hiểu rồi! Đây là pipeline Text-to-SQL, một loại pipeline khác hẳn với các pipeline trước.\n",
    "\n",
    "Các điểm chính:\n",
    "\n",
    "1. **Kết nối Database**:\n",
    "```python\n",
    "# Kết nối PostgreSQL\n",
    "self.engine = create_engine(f\"postgresql+psycopg2://{self.valves.DB_USER}:{self.valves.DB_PASSWORD}@{self.valves.DB_HOST}:{self.valves.DB_PORT}/{self.valves.DB_DATABASE}\")\n",
    "```\n",
    "\n",
    "2. **Cấu hình thông qua Valves**:\n",
    "```python\n",
    "self.valves = self.Valves(\n",
    "    DB_HOST: str,        # Host database\n",
    "    DB_PORT: str,        # Port database\n",
    "    DB_USER: str,        # Tên người dùng\n",
    "    DB_PASSWORD: str,    # Mật khẩu\n",
    "    DB_DATABASE: str,    # Tên database\n",
    "    DB_TABLE: str,       # Bảng cần truy vấn\n",
    "    OLLAMA_HOST: str,    # Host của Ollama\n",
    "    TEXT_TO_SQL_MODEL: str  # Model dùng để chuyển text thành SQL\n",
    ")\n",
    "```\n",
    "\n",
    "3. **Prompt Template đặc biệt**:\n",
    "- Hướng dẫn LLM tạo câu truy vấn SQL từ câu hỏi\n",
    "- Yêu cầu sử dụng DISTINCT để tránh trùng lặp\n",
    "- Giới hạn kết quả (mặc định 5 rows)\n",
    "- Chỉ truy vấn các cột cần thiết\n",
    "\n",
    "4. **Query Engine**:\n",
    "```python\n",
    "query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=[self.valves.DB_TABLE],\n",
    "    llm=llm,\n",
    "    embed_model=\"local\",\n",
    "    text_to_sql_prompt=text_to_sql_template,\n",
    "    streaming=True\n",
    ")\n",
    "```\n",
    "\n",
    "Khác biệt với các pipeline trước:\n",
    "- Làm việc với database thay vì tài liệu\n",
    "- Chuyển câu hỏi thành câu truy vấn SQL\n",
    "- Trả về kết quả từ database thay vì tìm kiếm trong văn bản\n",
    "\n",
    "Pipeline này phù hợp khi bạn muốn:\n",
    "- Truy vấn database bằng ngôn ngữ tự nhiên\n",
    "- Tự động tạo câu truy vấn SQL\n",
    "- Lấy dữ liệu có cấu trúc từ database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/BrainDriveAI/openwebui-pipelines?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository, **openwebui-pipelines**, contains custom pipelines designed to enhance the functionality of OpenWebUI, a framework for managing workflows. Developed by **BrainDriveAI**, the pipelines integrate advanced features such as memory management, YouTube transcript-based interaction, and modular workflows.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Chat with YouTube Pipeline**:\n",
    "   - **OpenAI Version**: Retrieves and summarizes YouTube video transcripts, performs Q&A, and integrates with OpenAI's GPT for natural language processing.\n",
    "   - **Ollama Version**: Provides similar features but uses Ollama’s local LLMs for enhanced privacy and cost efficiency.\n",
    "\n",
    "2. **Memory Pipelines**:\n",
    "   - **OpenAI + PostgreSQL**: Combines OpenAI embeddings with Supabase PostgreSQL for scalable, cloud-based long-term memory storage.\n",
    "   - **OpenAI + Neo4j**: A local-first memory solution with graph-based storage, implemented via Docker.\n",
    "   - **Ollama + Neo4j**: Fully local memory storage using Ollama's embeddings, prioritizing privacy and cost-effectiveness.\n",
    "\n",
    "### Installation and Setup:\n",
    "- Pipelines can be installed directly in OpenWebUI through the Admin Panel by pasting the GitHub URL.\n",
    "- Dockerized solutions (e.g., Neo4j pipelines) are supported using a provided `docker-compose.yml`.\n",
    "\n",
    "### Troubleshooting:\n",
    "- Issues like schema validation errors (e.g., `FieldValidatorDecoratorInfo.__init__`) can be resolved by upgrading dependencies, such as `pydantic`.\n",
    "\n",
    "### Resources and Licensing:\n",
    "- The project is licensed under the **MIT License** and provides references to documentation for integrated tools like OpenWebUI, PostgreSQL, Neo4j, OpenAI, and Ollama.\n",
    "\n",
    "The repository demonstrates a robust framework for extending OpenWebUI's capabilities with modern AI-powered workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MartianInGreen/OpenWebUI-Tools?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://openwebui.com/tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zohaib.me/extending-openwebui-using-pipelines/?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
