{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xem thử @examples xem mấy ô pipelines kia làm gì để hiển thị trên UI thì mình làm theo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ các code examples, tôi thấy có một số pattern chung để tạo pipeline hiển thị trên UI:\n",
    "\n",
    "1. Cấu trúc cơ bản của một Pipeline:\n",
    "\n",
    "```python\n",
    "class Pipeline:\n",
    "    # Định nghĩa các tham số cấu hình qua Valves\n",
    "    class Valves(BaseModel):\n",
    "        pipelines: List[str] = []  # Danh sách các pipeline được kết nối\n",
    "        priority: int = 0  # Độ ưu tiên của pipeline\n",
    "        # Các tham số cấu hình khác...\n",
    "\n",
    "    def __init__(self):\n",
    "        self.type = \"filter\"  # hoặc \"manifold\" cho provider\n",
    "        # Tùy chọn set id và name\n",
    "        self.name = \"My Pipeline Name\"\n",
    "        self.valves = self.Valves()\n",
    "```\n",
    "\n",
    "2. Các phương thức lifecycle cần implement:\n",
    "\n",
    "```python\n",
    "async def on_startup(self):\n",
    "    # Khởi tạo resources khi server start\n",
    "    print(f\"on_startup:{__name__}\")\n",
    "    pass\n",
    "\n",
    "async def on_shutdown(self):\n",
    "    # Cleanup resources khi server stop  \n",
    "    print(f\"on_shutdown:{__name__}\")\n",
    "    pass\n",
    "\n",
    "async def on_valves_updated(self):\n",
    "    # Xử lý khi cấu hình valves thay đổi\n",
    "    pass\n",
    "```\n",
    "\n",
    "3. Các loại pipeline chính:\n",
    "\n",
    "a. Filter Pipeline:\n",
    "- Dùng để xử lý/biến đổi messages trước/sau khi gửi tới model\n",
    "- Implement phương thức inlet() và outlet():\n",
    "\n",
    "```python\n",
    "async def inlet(self, body: dict, user: Optional[dict] = None) -> dict:\n",
    "    # Xử lý message trước khi gửi tới model\n",
    "    return body\n",
    "\n",
    "async def outlet(self, body: dict, user: Optional[dict] = None) -> dict:\n",
    "    # Xử lý response từ model trước khi trả về client\n",
    "    return body\n",
    "```\n",
    "\n",
    "b. Provider Pipeline: \n",
    "- Dùng để tích hợp các model AI\n",
    "- Implement phương thức pipe():\n",
    "\n",
    "```python\n",
    "def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, Generator, Iterator]:\n",
    "    # Xử lý request tới model provider\n",
    "    try:\n",
    "        # Gọi API của provider\n",
    "        response = requests.post(...)\n",
    "        \n",
    "        if body[\"stream\"]:\n",
    "            return response.iter_lines()  # Stream response\n",
    "        else:\n",
    "            return response.json()  # Non-stream response\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "```\n",
    "\n",
    "4. Metadata cho pipeline:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "title: My Pipeline\n",
    "author: author name  \n",
    "date: 2024-01-01\n",
    "version: 1.0\n",
    "license: MIT\n",
    "description: Pipeline description\n",
    "requirements: package1, package2\n",
    "environment_variables: ENV_VAR1, ENV_VAR2\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "5. Best practices:\n",
    "\n",
    "- Không hardcode pipeline ID, để tự động lấy từ tên file\n",
    "- Định nghĩa rõ các tham số cấu hình trong Valves\n",
    "- Xử lý lỗi và logging đầy đủ\n",
    "- Support cả stream và non-stream response\n",
    "- Cleanup resources trong on_shutdown()\n",
    "\n",
    "Để hiển thị trên UI, bạn cần:\n",
    "\n",
    "1. Đặt file pipeline trong thư mục examples/filters/ hoặc examples/pipelines/\n",
    "2. Implement đầy đủ các phương thức lifecycle\n",
    "3. Định nghĩa metadata và requirements\n",
    "4. Định nghĩa các tham số cấu hình trong Valves\n",
    "\n",
    "UI sẽ tự động load và hiển thị pipeline của bạn dựa trên các thông tin này.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "OK, tôi sẽ cập nhật file của bạn theo chuẩn pipeline để hiển thị tốt trên UI:\n",
    "\n",
    "```python:pipelines/qdant_pipeline_zohaibMeExtending.py\n",
    "\"\"\"\n",
    "title: Qdrant Vector Search Pipeline\n",
    "author: Zohaib\n",
    "date: 2025-01-10\n",
    "version: 1.0\n",
    "license: MIT\n",
    "description: Pipeline for semantic search using Qdrant Cloud\n",
    "requirements: requests, python-dotenv, llama-index, langchain, langchain-community, qdrant-client\n",
    "environment_variables: QDRANT_API_URL, QDRANT_API_KEY, QDRANT_COLLECTION, HUGGINGFACE_API_KEY\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Union, Optional, Generator, Iterator\n",
    "from pydantic import BaseModel\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Pipeline:\n",
    "    class Valves(BaseModel):\n",
    "        model_config = {\n",
    "            \"arbitrary_types_allowed\": True\n",
    "        }\n",
    "        \n",
    "        pipelines: List[str] = []  # Connected pipelines\n",
    "        priority: int = 0  # Pipeline priority\n",
    "        QDRANT_API_URL: str = os.getenv(\"QDRANT_API_URL\", \"\")\n",
    "        QDRANT_API_KEY: str = os.getenv(\"QDRANT_API_KEY\", \"\")\n",
    "        QDRANT_COLLECTION: str = os.getenv(\"QDRANT_COLLECTION\", \"\")\n",
    "        HUGGINGFACE_API_KEY: str = os.getenv(\"HUGGINGFACE_API_KEY\", \"\")\n",
    "        EMBEDDINGS_MODEL_NAME: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.type = \"filter\"  # or \"manifold\" for provider\n",
    "        self.name = \"Qdrant Vector Search\"\n",
    "        self.valves = self.Valves()\n",
    "        self._initialize_clients()\n",
    "\n",
    "    def _initialize_clients(self):\n",
    "        try:\n",
    "            print(\"Initializing Pipeline...\")\n",
    "            # Initialize Hugging Face embeddings\n",
    "            self.embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "                api_key=self.valves.HUGGINGFACE_API_KEY,\n",
    "                model_name=self.valves.EMBEDDINGS_MODEL_NAME\n",
    "            )\n",
    "\n",
    "            # Initialize Qdrant client\n",
    "            self.qdrant_client = QdrantClient(\n",
    "                url=self.valves.QDRANT_API_URL,\n",
    "                api_key=self.valves.QDRANT_API_KEY\n",
    "            )\n",
    "\n",
    "            # Test connection\n",
    "            self.qdrant_client.get_collections()\n",
    "            print(\"Successfully connected to Qdrant\")\n",
    "            print(\"Pipeline initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize pipeline: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def on_startup(self):\n",
    "        print(f\"Starting {self.name} pipeline...\")\n",
    "        self._initialize_clients()\n",
    "\n",
    "    async def on_shutdown(self):\n",
    "        print(f\"Shutting down {self.name} pipeline...\")\n",
    "        if hasattr(self, 'qdrant_client'):\n",
    "            self.qdrant_client.close()\n",
    "\n",
    "    async def on_valves_updated(self):\n",
    "        print(\"Valves configuration updated\")\n",
    "        self._initialize_clients()\n",
    "\n",
    "    async def inlet(self, body: dict, user: Optional[dict] = None) -> dict:\n",
    "        \"\"\"Pre-process incoming messages\"\"\"\n",
    "        try:\n",
    "            if \"messages\" in body:\n",
    "                last_message = body[\"messages\"][-1][\"content\"]\n",
    "                vector = self.generate_embedding(last_message)\n",
    "                body[\"vector\"] = vector\n",
    "            return body\n",
    "        except Exception as e:\n",
    "            print(f\"Inlet error: {e}\")\n",
    "            return body\n",
    "\n",
    "    async def outlet(self, body: dict, user: Optional[dict] = None) -> dict:\n",
    "        \"\"\"Post-process outgoing messages\"\"\"\n",
    "        return body\n",
    "\n",
    "    def generate_embedding(self, query_text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding vector for query text\"\"\"\n",
    "        try:\n",
    "            embedding_vector = self.embeddings.embed_query(query_text)\n",
    "            return embedding_vector\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_k: int = 5) -> dict:\n",
    "        \"\"\"Search Qdrant collection\"\"\"\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.valves.QDRANT_COLLECTION,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "            return {\"result\": results}\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _format_results(self, matches: list) -> str:\n",
    "        \"\"\"Format search results for display\"\"\"\n",
    "        results = []\n",
    "        for idx, match in enumerate(matches, 1):\n",
    "            payload = match.payload\n",
    "            score = match.score\n",
    "            content = payload.get(\"content\", \"No content\")\n",
    "            results.append(f\"{idx}. [Score: {score:.2f}] {content}\")\n",
    "        return \"\\n\\n\".join(results)\n",
    "\n",
    "    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Union[str, dict]:\n",
    "        \"\"\"Main pipeline processing\"\"\"\n",
    "        try:\n",
    "            print(f\"Processing message: {user_message}\")\n",
    "            \n",
    "            # Generate embedding\n",
    "            query_vector = self.generate_embedding(user_message)\n",
    "            if not query_vector:\n",
    "                return {\"response\": \"Failed to generate embedding\"}\n",
    "\n",
    "            # Search vectors\n",
    "            results = self.search_vectors(query_vector)\n",
    "            if \"error\" in results:\n",
    "                return {\"response\": f\"Search error: {results['error']}\"}\n",
    "\n",
    "            # Format results\n",
    "            matches = results.get(\"result\", [])\n",
    "            if not matches:\n",
    "                return {\"response\": \"No relevant results found\"}\n",
    "\n",
    "            # Format response for UI display\n",
    "            formatted_response = {\n",
    "                \"response\": self._format_results(matches),\n",
    "                \"matches\": [match.dict() for match in matches],\n",
    "                \"total\": len(matches)\n",
    "            }\n",
    "            \n",
    "            print(f\"Returning response: {formatted_response}\")\n",
    "            return formatted_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline error: {str(e)}\")\n",
    "            return {\"response\": f\"Error: {str(e)}\"}\n",
    "```\n",
    "\n",
    "Các thay đổi chính:\n",
    "\n",
    "1. **Metadata đầy đủ**:\n",
    "   - Thêm thông tin về environment variables\n",
    "   - Mô tả rõ ràng về chức năng\n",
    "\n",
    "2. **Cấu trúc Pipeline chuẩn**:\n",
    "   - Thêm lifecycle methods (on_startup, on_shutdown, on_valves_updated)\n",
    "   - Thêm inlet/outlet cho pre/post processing\n",
    "   - Cấu hình Valves với các tham số mặc định\n",
    "\n",
    "3. **Cải thiện xử lý kết quả**:\n",
    "   - Thêm phương thức _format_results\n",
    "   - Response format chuẩn cho UI\n",
    "\n",
    "4. **Resource Management**:\n",
    "   - Cleanup resources trong on_shutdown\n",
    "   - Khởi tạo lại clients khi valves update\n",
    "\n",
    "5. **Error Handling & Logging**:\n",
    "   - Xử lý lỗi chi tiết hơn\n",
    "   - Logging rõ ràng cho debug\n",
    "\n",
    "Bây giờ pipeline sẽ:\n",
    "- Hiển thị đúng trên UI\n",
    "- Có thể cấu hình thông qua UI\n",
    "- Xử lý và hiển thị kết quả tốt hơn\n",
    "- Quản lý resources hiệu quả\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 filter -)) từ pipeline mà nhầm sang filter là sao ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Trong OpenWebUI, có 2 loại pipeline chính:\n",
    "\n",
    "1. **Filter Pipeline** (`type = \"filter\"`):\n",
    "- Dùng để xử lý/biến đổi messages trước/sau khi gửi tới model\n",
    "- Sử dụng 2 phương thức chính: `inlet()` (pre-processing) và `outlet()` (post-processing)\n",
    "- Thường dùng để:\n",
    "  - Thêm context từ vector DB\n",
    "  - Kiểm duyệt nội dung\n",
    "  - Format messages\n",
    "  - Thêm metadata\n",
    "\n",
    "2. **Provider Pipeline** (`type = \"manifold\"`):\n",
    "- Dùng để tích hợp các model AI\n",
    "- Sử dụng phương thức `pipe()` để xử lý request tới model\n",
    "- Thường dùng để:\n",
    "  - Kết nối tới các model như GPT, Claude, Llama...\n",
    "  - Xử lý streaming responses\n",
    "  - Handle model-specific logic\n",
    "\n",
    "Trong trường hợp Qdrant pipeline của chúng ta, nó đang hoạt động như một filter để:\n",
    "1. Lấy message từ user\n",
    "2. Tìm kiếm context liên quan trong Qdrant\n",
    "3. Thêm context vào prompt trước khi gửi tới model\n",
    "\n",
    "Để sử dụng filter hiệu quả hơn, ta có thể sửa lại:\n",
    "\n",
    "```python:pipelines/qdant_pipeline_zohaibMeExtending.py\n",
    "async def inlet(self, body: dict, user: Optional[dict] = None) -> dict:\n",
    "    \"\"\"Pre-process incoming messages\"\"\"\n",
    "    try:\n",
    "        if \"messages\" in body:\n",
    "            # Lấy message cuối cùng từ user\n",
    "            last_message = body[\"messages\"][-1][\"content\"]\n",
    "            print(f\"Processing message: {last_message}\")\n",
    "            \n",
    "            # Tìm context từ Qdrant\n",
    "            query_vector = self.generate_embedding(last_message)\n",
    "            results = self.search_vectors(query_vector)\n",
    "            \n",
    "            if \"error\" not in results:\n",
    "                matches = results.get(\"result\", [])\n",
    "                if matches:\n",
    "                    # Format context\n",
    "                    context = self._format_results(matches)\n",
    "                    \n",
    "                    # Thêm context vào system message\n",
    "                    system_message = {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": f\"Use this context to help answer the question:\\n\\n{context}\\n\\n\"\n",
    "                    }\n",
    "                    body[\"messages\"].insert(0, system_message)\n",
    "                    \n",
    "                    print(f\"Added context from {len(matches)} matches\")\n",
    "                    \n",
    "        return body\n",
    "    except Exception as e:\n",
    "        print(f\"Inlet error: {e}\")\n",
    "        return body\n",
    "\n",
    "async def outlet(self, body: dict, user: Optional[dict] = None) -> dict:\n",
    "    \"\"\"Post-process outgoing messages\"\"\"\n",
    "    try:\n",
    "        if \"choices\" in body:\n",
    "            # Có thể thêm xử lý response ở đây\n",
    "            pass\n",
    "        return body\n",
    "    except Exception as e:\n",
    "        print(f\"Outlet error: {e}\")\n",
    "        return body\n",
    "```\n",
    "\n",
    "Flow hoạt động:\n",
    "\n",
    "1. User gửi message\n",
    "2. `inlet()` được gọi:\n",
    "   - Tạo embedding từ message\n",
    "   - Tìm context trong Qdrant\n",
    "   - Thêm context vào prompt\n",
    "3. Message được gửi tới model\n",
    "4. Model trả về response\n",
    "5. `outlet()` được gọi để xử lý response (nếu cần)\n",
    "6. Response được trả về user\n",
    "\n",
    "Cách này giúp:\n",
    "- Tách biệt logic tìm kiếm và xử lý model\n",
    "- Dễ dàng thêm/sửa context\n",
    "- Có thể xử lý cả input và output\n",
    "- Tương thích với nhiều model khác nhau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
